import streamlit as st
from diffusers import StableDiffusionPipeline
import torch
from PIL import Image

# Configuration de l'application Streamlit
st.set_page_config(page_title="Projet IA - GÃ©nÃ©ration d'Images", layout="wide")
st.title("ğŸ–¼ï¸ GÃ©nÃ©ration d'Images avec Stable Diffusion v1.5")

# ğŸ¨ AmÃ©lioration de l'interface utilisateur
st.markdown("""
    <style>
    body {
        background-color: #f0f2f6;
        font-family: Arial, sans-serif;
    }
    .stTextInput > div > input {
        border-radius: 10px;
        padding: 10px;
    }
    .stButton > button {
        background-color: #4CAF50;
        color: white;
        border-radius: 10px;
        padding: 10px 20px;
    }
    </style>
""", unsafe_allow_html=True)

# Chargement du modÃ¨le (cela peut prendre un peu de temps la premiÃ¨re fois)
@st.cache_resource
def load_model():
    model_id = "runwayml/stable-diffusion-v1-5"  # Nom correct du modÃ¨le
    try:
        pipe = StableDiffusionPipeline.from_pretrained(
            model_id, 
            torch_dtype=torch.float16,
            variant="fp16",
            use_auth_token=True  # Permet l'utilisation de ton Token Hugging Face
        )
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        pipe.to(device)
        return pipe
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement du modÃ¨le : {e}")
        return None

pipe = load_model()

# ğŸ“Œ Message d'avertissement si on utilise le CPU
if torch.backends.mps.is_available():
    st.success("âœ… GPU MPS dÃ©tectÃ© ! Le modÃ¨le est chargÃ© sur GPU.")
else:
    st.warning("âš ï¸ Aucun GPU dÃ©tectÃ©. Le modÃ¨le est chargÃ© sur le CPU, ce qui peut Ãªtre lent.")

# ğŸ¯ EntrÃ©e utilisateur
st.subheader("ğŸ“ Prompt de gÃ©nÃ©ration")
prompt = st.text_input("ğŸ’¬ Entrez votre prompt :", value="A beautiful sunset over a mountain landscape, photorealistic")

# ğŸ“Œ ParamÃ¨tres avancÃ©s
st.sidebar.header("ğŸ”§ ParamÃ¨tres avancÃ©s")
guidance_scale = st.sidebar.slider("ğŸ› ProximitÃ© avec le prompt (Plus la valeur est haute plus la gÃ©nÃ©ration sera proche du prompt)", 1.0, 20.0, 7.5)
num_inference_steps = st.sidebar.slider("ğŸ“ Nomre d'Ã©tapes pour affiner l'image (plus d'Ã©tapes = plus longuÃ© gÃ©nÃ©ration mais plus proche du prompt)", 10, 100, 50)

# ğŸ¨ Bouton pour gÃ©nÃ©rer l'image
if st.button("ğŸ¨ GÃ©nÃ©rer l'image"):
    if not prompt.strip():
        st.warning("âš ï¸ Veuillez entrer un prompt valide.")
    else:
        with st.spinner("ğŸ–Œï¸ GÃ©nÃ©ration en cours..."):
            try:
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                pipe.to(device)
                
                image = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]

                # ğŸ“Œ Affichage de l'image gÃ©nÃ©rÃ©e
                st.image(image, caption="âœ… Image gÃ©nÃ©rÃ©e avec succÃ¨s", use_container_width=True)
            except Exception as e:
                st.error(f"âŒ Erreur lors de la gÃ©nÃ©ration : {e}")